{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOpeh5mbChXTWiaWNL/1ni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tushaam/JAX-tutorials/blob/main/JAX_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgzbwFIhQfTt",
        "outputId": "af62594c-9194-4425-e518-f8a84d716fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda] in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Collecting jax[cuda]\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax[cuda])\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax[cuda]) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax[cuda]) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax[cuda]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax[cuda]) (1.14.1)\n",
            "Collecting jax-cuda12-plugin<=0.5.3,>=0.5.3 (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda])\n",
            "  Downloading jax_cuda12_plugin-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting jax-cuda12-pjrt==0.5.3 (from jax-cuda12-plugin<=0.5.3,>=0.5.3->jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda])\n",
            "  Downloading jax_cuda12_pjrt-0.5.3-py3-none-manylinux2014_x86_64.whl.metadata (492 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (12.5.82)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.6.85 (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda])\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (9.3.0.75)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (11.2.3.61)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (12.5.1.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda\"->jax[cuda]) (12.5.82)\n",
            "Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl (105.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.5.3-cp311-cp311-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.5.3-py3-none-manylinux2014_x86_64.whl (104.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.7/104.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.5.3-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, nvidia-cuda-nvcc-cu12, jax-cuda12-plugin, jaxlib, jax\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.5.1\n",
            "    Uninstalling jax-cuda12-pjrt-0.5.1:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.5.1\n",
            "  Attempting uninstall: nvidia-cuda-nvcc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvcc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvcc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.5.82\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.5.1\n",
            "    Uninstalling jax-cuda12-plugin-0.5.1:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.5.1\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "Successfully installed jax-0.5.3 jax-cuda12-pjrt-0.5.3 jax-cuda12-plugin-0.5.3 jaxlib-0.5.3 nvidia-cuda-nvcc-cu12-12.8.93\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "Jqbe-0zQRHa-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_8_L_OIW5gq",
        "outputId": "2f2acbc2-1c42-4781-960d-642353b939d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CudaDevice(id=0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us try to first create a JAX array now"
      ],
      "metadata": {
        "id": "MNqini48XPxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.array([5, 10, 15])\n",
        "\n",
        "\n",
        "print(\"x =\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVa8p8C4XYqW",
        "outputId": "8acdf31f-94e1-4e35-981f-112e8fe38df3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = [ 5 10 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for value in range(2):\n",
        "  x= value + x**2\n",
        "\n",
        "print(\"x=\",x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGSEM5kFXe9k",
        "outputId": "c583c606-4a3e-4470-b3fb-b0d8ac8268f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x= [  626 10001 50626]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*So what did we do here?* we just iterated from 0 to 1 and updated each element of x by squaring it and adding value to it\n",
        "\n",
        "**first iteration:**  x= 0 + [5x5,10x10,15x15]...now x=[25,100,225]\n",
        "\n",
        "**second iteration:** x= 1 + [25x25, 100x100, 225x225]..now x=[626,10001,50626]"
      ],
      "metadata": {
        "id": "QrCivvltYg2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us now try to compute sin and cos of the array x elements"
      ],
      "metadata": {
        "id": "kyYp2DhEaGkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sin_x = jnp.sin(x)\n",
        "cos_x = jnp.cos(x)\n",
        "\n",
        "print(\"Sin(x) for the array x are:\",sin_x)\n",
        "print(\"Cos(x) for the array x are:\",cos_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY-5BzeoZaFb",
        "outputId": "5ebba1b8-d5c4-4479-fa75-3cd5f9b80878"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sin(x) for the array x are: [-0.73323137 -0.9663353   0.6929788 ]\n",
            "Cos(x) for the array x are: [-0.67997926 -0.2572861  -0.720958  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us come to the *magical* part of JAX\n",
        "\n",
        "# **AUTOMATIC DIFFERENTIATION**"
      ],
      "metadata": {
        "id": "1AWOef4XankI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func(x):\n",
        "  return jnp.sum(x ** 2 + 3*x)\n",
        "\n",
        "grad_func = jax.grad(func)\n",
        "\n",
        "x = jnp.array([5.0, 10.0, 15.0])\n",
        "\n",
        "grad = grad_func(x)\n",
        "print(\"Gradient of x^2 is:\", grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcU2HywTagXx",
        "outputId": "06de5327-0d1e-4f28-bed7-7fc362a64485"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of x^2 is: [13. 23. 33.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This automatic differentition ability of JAX is amazing especially in large neural network architectures where computing higher order gradients can be cumbersome.\n",
        "\n",
        "suppose we want to obtain 2nd order differentiation...we can just use this jax.grad() function to the 1st gradient obtained.ğŸ˜€"
      ],
      "metadata": {
        "id": "t6ojA-cbb8qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us now use another remarkable concept of JAX called **JIT**(just in time)"
      ],
      "metadata": {
        "id": "jVYEDU-Qdi07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What exactly JIT does is it speeds up the computations remarkably faster compared to numpy operations. Let us write a code to compare the time for an operation implemented with numpy and then JAX"
      ],
      "metadata": {
        "id": "ZHsHPTz3pwMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from jax import jit\n",
        "\n",
        "def slow_function(x):\n",
        "    return jnp.sin(x) ** 2 + jnp.cos(x) ** 2\n",
        "\n",
        "\n",
        "fast_function = jit(slow_function)\n",
        "\n",
        "x = jnp.linspace(0, 1000, 1_000_000)\n",
        "\n",
        "start = time.time()\n",
        "slow_function(x)\n",
        "print(\"Without jit:\", time.time() - start)\n",
        "\n",
        "start = time.time()\n",
        "fast_function(x)\n",
        "print(\"With jit (1st call - compile):\", time.time() - start)\n",
        "\n",
        "start = time.time()\n",
        "fast_function(x)\n",
        "print(\"With jit (2nd call):\", time.time() - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o-qwLQFbJDV",
        "outputId": "2fe1e97a-8001-41cb-8d67-48f0ef9e3576"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without jit: 0.35568976402282715\n",
            "With jit (1st call - compile): 0.22977805137634277\n",
            "With jit (2nd call): 0.00046706199645996094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WOHOOOOOO!! Isn't it fascinating????\n",
        "\n",
        "The 2nd call of function using JIT was literally **99.87%** faster than that with numpy"
      ],
      "metadata": {
        "id": "jIuUd_9WttYd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BTItxUFud42X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}